{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Test_model.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ASpIhHSCq7Pw","outputId":"0d640ef1-bd7a-495e-e93d-c4f34b875e3b","executionInfo":{"status":"ok","timestamp":1648200716173,"user_tz":-420,"elapsed":22682,"user":{"displayName":"Nghĩa Ngọ Doãn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13982214725351301935"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install elasticsearch==7.9.1"],"metadata":{"id":"DCostpGJVbYd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d4252004-468f-4019-9ff7-10c909da0b43","executionInfo":{"status":"ok","timestamp":1648200720729,"user_tz":-420,"elapsed":4568,"user":{"displayName":"Nghĩa Ngọ Doãn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13982214725351301935"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting elasticsearch==7.9.1\n","  Downloading elasticsearch-7.9.1-py2.py3-none-any.whl (219 kB)\n","\u001b[?25l\r\u001b[K     |█▌                              | 10 kB 22.5 MB/s eta 0:00:01\r\u001b[K     |███                             | 20 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 30 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 40 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 51 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 61 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 71 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 81 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 92 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 219 kB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch==7.9.1) (1.24.3)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch==7.9.1) (2021.10.8)\n","Installing collected packages: elasticsearch\n","Successfully installed elasticsearch-7.9.1\n"]}]},{"cell_type":"code","source":["cd /content/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EYvLErOTLBBA","executionInfo":{"status":"ok","timestamp":1648200720731,"user_tz":-420,"elapsed":26,"user":{"displayName":"Nghĩa Ngọ Doãn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13982214725351301935"}},"outputId":"3be34b2b-800f-4c98-b52f-11d2d8580786"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["from elasticsearch import Elasticsearch\n","import time"],"metadata":{"id":"nLSw1wQYVkk3","executionInfo":{"status":"ok","timestamp":1648200721235,"user_tz":-420,"elapsed":518,"user":{"displayName":"Nghĩa Ngọ Doãn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13982214725351301935"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["%%bash\n","\n","wget -q https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz\n","wget -q https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512\n","tar -xzf elasticsearch-oss-7.9.2-linux-x86_64.tar.gz\n","sudo chown -R daemon:daemon elasticsearch-7.9.2/\n","shasum -a 512 -c elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512 "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"37bFbAHgVkh-","outputId":"ef9a0ef4-2f6c-484d-c4f9-cf17234900a1","executionInfo":{"status":"ok","timestamp":1648200737895,"user_tz":-420,"elapsed":16671,"user":{"displayName":"Nghĩa Ngọ Doãn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13982214725351301935"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["elasticsearch-oss-7.9.2-linux-x86_64.tar.gz: OK\n"]}]},{"cell_type":"code","source":["%%bash --bg\n","\n","sudo -H -u daemon elasticsearch-7.9.2/bin/elasticsearch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fPpaDcttVke_","outputId":"82862e75-df13-411b-c2bc-45d4648cc840","executionInfo":{"status":"ok","timestamp":1648200737896,"user_tz":-420,"elapsed":27,"user":{"displayName":"Nghĩa Ngọ Doãn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13982214725351301935"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting job # 0 in a separate thread.\n"]}]},{"cell_type":"code","source":["# Sleep for few seconds to let the instance start.\n","time.sleep(10)"],"metadata":{"id":"iHEwPLYrWILg","executionInfo":{"status":"ok","timestamp":1648200748016,"user_tz":-420,"elapsed":10144,"user":{"displayName":"Nghĩa Ngọ Doãn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13982214725351301935"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["%%bash\n","\n","ps -ef | grep elasticsearch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RW_L3C6CWRUW","outputId":"d49c6a83-2dfd-4b30-d0e5-890945453118","executionInfo":{"status":"ok","timestamp":1648200748018,"user_tz":-420,"elapsed":12,"user":{"displayName":"Nghĩa Ngọ Doãn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13982214725351301935"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["root         322     320  0 09:32 ?        00:00:00 sudo -H -u daemon elasticsearch-7.9.2/bin/elasticsearch\n","daemon       323     322 83 09:32 ?        00:00:08 /content/elasticsearch-7.9.2/jdk/bin/java -Xshare:auto -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -XX:+ShowCodeDetailsInExceptionMessages -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dio.netty.allocator.numDirectArenas=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true -Djava.locale.providers=SPI,COMPAT -Xms1g -Xmx1g -XX:+UseG1GC -XX:G1ReservePercent=25 -XX:InitiatingHeapOccupancyPercent=30 -Djava.io.tmpdir=/tmp/elasticsearch-3836446945042554888 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=data -XX:ErrorFile=logs/hs_err_pid%p.log -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m -XX:MaxDirectMemorySize=536870912 -Des.path.home=/content/elasticsearch-7.9.2 -Des.path.conf=/content/elasticsearch-7.9.2/config -Des.distribution.flavor=oss -Des.distribution.type=tar -Des.bundled_jdk=true -cp /content/elasticsearch-7.9.2/lib/* org.elasticsearch.bootstrap.Elasticsearch\n","root         552     550  0 09:32 ?        00:00:00 grep elasticsearch\n"]}]},{"cell_type":"code","source":["%%bash\n","\n","curl -sX GET \"localhost:9200/\""],"metadata":{"id":"3E3JhWMKWVWn","executionInfo":{"status":"ok","timestamp":1648200748482,"user_tz":-420,"elapsed":472,"user":{"displayName":"Nghĩa Ngọ Doãn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13982214725351301935"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## Extract data from wiki dump"],"metadata":{"id":"a99iiBxmaBU2"}},{"cell_type":"code","source":["cd /content/drive/MyDrive/ColabNotebooks/NLP/Data_wiki/data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u5wNJz0iZeUO","outputId":"eed4f608-15a2-4fe6-933a-9c0ee0ab0f1b","executionInfo":{"status":"ok","timestamp":1648200960441,"user_tz":-420,"elapsed":324,"user":{"displayName":"Nghĩa Ngọ Doãn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13982214725351301935"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ColabNotebooks/NLP/Data_wiki/data\n"]}]},{"cell_type":"code","source":["import io \n","import json\n","import pickle\n","import base64\n","import os\n","from tqdm import tqdm\n","import re\n","import codecs\n","from elasticsearch.helpers import streaming_bulk\n","from elasticsearch import Elasticsearch"],"metadata":{"id":"cy9rSxpZZnTO","executionInfo":{"status":"ok","timestamp":1648200748485,"user_tz":-420,"elapsed":10,"user":{"displayName":"Nghĩa Ngọ Doãn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13982214725351301935"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["## Load mention, entity, alias\n","\n","with open('./output/mentions.json', 'r') as f:\n","    mentions = json.load(f)\n","\n","with open('./output/entities.json', 'r') as f:\n","    entities = json.load(f)\n","\n","with open('./output/mentions_entities.json', 'r') as f:\n","    mentions_entities = json.load(f)\n","\n","with open('./output/alias.pkl', 'rb') as f:\n","    alias = pickle.load(f)\n","\n","with open('./output/redirects.pkl', 'rb') as f:\n","    redirects = pickle.load(f)\n","\n","with open('./output/id2title.pkl', 'rb') as f:\n","    id2title = pickle.load(f)"],"metadata":{"id":"PiADGhR9naZ0","executionInfo":{"status":"ok","timestamp":1648200981884,"user_tz":-420,"elapsed":17574,"user":{"displayName":"Nghĩa Ngọ Doãn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13982214725351301935"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["len(mentions), len(entities), len(mentions_entities), len(alias), len(redirects), len(id2title)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UHyC918ocVsY","outputId":"a2651ea9-112f-4326-f558-d3745403d58a","executionInfo":{"status":"ok","timestamp":1648200981885,"user_tz":-420,"elapsed":16,"user":{"displayName":"Nghĩa Ngọ Doãn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13982214725351301935"}}},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(343842, 1269933, 404546, 1269933, 1270130, 1269933)"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["## Elasticsearch proccess"],"metadata":{"id":"7wtr6FrifDJa"}},{"cell_type":"code","source":["ES_NODES = \"http://localhost:9200\"\n","def create_index(index, client):\n","    client.indices.create(\n","        index=index,\n","        body={\n","            \"settings\": {\"number_of_shards\": 1},\n","            \"mappings\": {\n","                \"properties\": {\n","                    \"entity\": {\n","                        \"type\": \"keyword\", \n","                        \"fields\": {\n","                            \"length\": { \n","                                \"type\": \"token_count\",\n","                                \"analyzer\": \"standard\"\n","                            }\n","                        }\n","                    },\n","                    \"alias\": {\"type\": \"text\"},\n","                    \"redirects\": {\"type\": \"text\"},\n","                }\n","            },\n","        },\n","    )\n","\n","def generate_action():\n","  \n","    for i, id in enumerate(alias):\n","        if id in alias:\n","            als = [e for e in alias[id]]\n","        else:\n","            als = []\n","        if id in redirects:\n","            red = [e for e in redirects[id]]\n","        else:\n","            red = []\n","\n","        als.append(id2title[id])\n","        red.append(id2title[id])\n","        als = list(set(als))\n","        red = list(set(red))\n","        \n","        als = ' '.join([re.sub(r'[\\/\\\\~\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\-\\+\\[\\]\\\"\\'\\:\\`\\{\\}]', ' ', m) for m in als])\n","        red = ' '.join([re.sub(r'[\\/\\\\~\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\-\\+\\[\\]\\\"\\'\\:\\`\\{\\}]', ' ', m) for m in red])\n","\n","        yield {'entity': id2title[id], 'alias': als, 'redirects': red}\n","\n","def index_es_data(index):\n","    index = 'candidate'\n","    client = Elasticsearch(hosts = [ES_NODES])\n","    if client.indices.exists(index=index):\n","        print(\"deleting the '{}' index.\".format(index))\n","        res = client.indices.delete(index=index)\n","        print(\"Response from server: {}\".format(res))\n","  \n","    create_index(index, client)\n","\n","    successes = 0\n","\n","    for ok, action in streaming_bulk(\n","        client=client, index=index, actions=generate_action(),\n","    ):\n","        successes += ok\n","\n","    print(successes)"],"metadata":{"id":"k_8pqgfY0Aym","executionInfo":{"status":"ok","timestamp":1648200981886,"user_tz":-420,"elapsed":13,"user":{"displayName":"Nghĩa Ngọ Doãn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13982214725351301935"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["index_es_data('candidate')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eNKAq0K8x_iY","outputId":"2d79b0fb-926e-4a83-a4c3-36126c075b68","executionInfo":{"status":"ok","timestamp":1648201129865,"user_tz":-420,"elapsed":147991,"user":{"displayName":"Nghĩa Ngọ Doãn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13982214725351301935"}}},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["1269933\n"]}]},{"cell_type":"markdown","source":["## Generate Candidate"],"metadata":{"id":"2rJZLmA9soLM"}},{"cell_type":"code","source":["!pip install elasticsearch_dsl"],"metadata":{"id":"kd8hebs-t1U3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from elasticsearch_dsl import Search"],"metadata":{"id":"ZLc2ly2_sqNC","executionInfo":{"status":"ok","timestamp":1648201133321,"user_tz":-420,"elapsed":30,"user":{"displayName":"Nghĩa Ngọ Doãn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13982214725351301935"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["client = Elasticsearch()\n","s = Search(using=client, index='candidate').extra(size=20)"],"metadata":{"id":"lwGHpp2sstAQ","executionInfo":{"status":"ok","timestamp":1648201133321,"user_tz":-420,"elapsed":25,"user":{"displayName":"Nghĩa Ngọ Doãn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13982214725351301935"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["cd /content/drive/MyDrive/ColabNotebooks/NLP/Code_RunTeminal/EntityLinking/code"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fVqNvvrJ-6Zz","executionInfo":{"status":"ok","timestamp":1648201133322,"user_tz":-420,"elapsed":22,"user":{"displayName":"Nghĩa Ngọ Doãn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13982214725351301935"}},"outputId":"330b13be-51a3-471e-8b27-b7b8cbd085b3"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ColabNotebooks/NLP/Code_RunTeminal/EntityLinking/code\n"]}]},{"cell_type":"code","source":["from json import encoder\n","import pickle\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","# from EL_Dataset import CustomerDataset\n","from torch.utils.data import DataLoader\n","from sklearn.metrics import accuracy_score\n","from tqdm import tqdm\n","import torch.nn.functional as F\n","\n","from lstm import *\n","from mlp import *\n","from loss_margin import MyMarginLoss\n","from encoder import Encoder\n","from model_el import Entity_Linking"],"metadata":{"id":"ZIE0pc5S7mRn","executionInfo":{"status":"ok","timestamp":1648202017683,"user_tz":-420,"elapsed":322,"user":{"displayName":"Nghĩa Ngọ Doãn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13982214725351301935"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["def load_data():\n","\n","    ## Load word2id and char2id\n","    with open('../data/word2id.pkl', 'rb') as f:\n","        word2id = pickle.load(f)   \n","    with open('../data/char2id.pkl', 'rb') as f:\n","        char2id = pickle.load(f) \n","\n","    ## Load sentences\n","    with open('../data/sentences.pkl', 'rb') as f:\n","        sentences = pickle.load(f)\n","\n","    ## Load candidate elasticsearch\n","    with open('../data/candidate_elasticsearch_alias_top20_new.pkl', 'rb') as f:\n","        candidate_elsearch = pickle.load(f)\n","\n","    ## Load candidate prob \n","    with open('../data/output_e_give_m.pkl', 'rb') as f:\n","        candidate_prob = pickle.load(f)\n","\n","    ## Load samples\n","    with open('../data/samples_550000.pkl', 'rb') as f:\n","        samples = pickle.load(f)\n","\n","    ## Data test\n","    with open('../data/sample_test.pkl', 'rb') as f:\n","        samples_test = pickle.load(f)\n","    with open('../data/sentences_test.pkl', 'rb') as f:\n","        sentences_test = pickle.load(f)\n","\n","    ## Load summary \n","    with open('../data/summary.pkl', 'rb') as f:\n","        summary = pickle.load(f)\n","\n","    return word2id, char2id, sentences, candidate_elsearch, candidate_prob, samples, samples_test, sentences_test, summary\n","\n","def dataloader(sample_dataset, batch_size=64, train=True):\n","    \n","    if train:\n","        data = DataLoader(sample_dataset, batch_size=batch_size,shuffle=True)\n","\n","    else:\n","        data = DataLoader(sample_dataset, batch_size=batch_size, shuffle=False)\n","\n","    return data\n","\n","def evaluate(data_loaders):\n","    with torch.no_grad():\n","        loss_eval = 0\n","        predict = []\n","        labels = []\n","        for batch in tqdm(data_loaders):\n","            index_candidates, index_mentions, mask_mentions, mask_candidates, index_sentence, index_summary, char_start, char_end, idx_entities = batch\n","\n","            model.eval()\n","            score_candidate = model(index_candidates, index_mentions, index_sentence, index_summary)\n","            score_candidate = F.softmax(score_candidate, dim=-1)\n","            loss_eval += loss(score_candidate, idx_entities, mask_mentions, mask_candidates)\n","\n","            pred = torch.argmax(score_candidate, dim=2)\n","            pred = torch.masked_select(pred, mask_mentions).tolist()\n","            label = torch.masked_select(idx_entities, mask_mentions).tolist()\n","\n","            predict.extend(pred)\n","            labels.extend(label)\n","        print(\"Label: \", labels)\n","        print(\"Predict: \", predict)\n","        acc = accuracy_score(labels, predict)\n","\n","    return loss_eval, acc"],"metadata":{"id":"0Eq9jREd-vZF","executionInfo":{"status":"ok","timestamp":1648201640031,"user_tz":-420,"elapsed":321,"user":{"displayName":"Nghĩa Ngọ Doãn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13982214725351301935"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","import re\n","\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","\n","class CustomerDataset(Dataset):\n","    \n","    def __init__(self, samples, sentences, summary, num_mention, num_candidate, \n","                 max_length_seq_char, max_length_seq_sence, max_length_word,\n","                 char2id, word2id, candidate_prob, candidate_elsearch, device='cpu', search=None, train=True):\n","        super().__init__()\n","        self.samples = samples\n","        self.sentences = sentences\n","        self.summary = summary\n","        self.num_mention = num_mention\n","        self.num_candidate = num_candidate\n","        self.max_length_seq_char = max_length_seq_char \n","        self.max_length_seq_sence = max_length_seq_sence \n","        self.max_length_word = max_length_word\n","        self.char2id = char2id \n","        self.word2id = word2id\n","        self.candidate_prob = candidate_prob\n","        self.candidate_elsearch = candidate_elsearch\n","        self.search = search\n","        self.train = train\n","        self.device = device\n","    \n","    def __len__(self):\n","        return len(self.samples)\n","    \n","    def __getitem__(self, idx):\n","        \n","        # get data from index\n","        sample = self.samples[idx]\n","        mentions = sample['mention'][:self.num_mention]\n","        entities = sample['entity'][:self.num_mention]\n","        char_start = sample['char_start'][:self.num_mention]\n","        char_end = sample['char_end'][:self.num_mention]\n","        sentence_id = sample['id']\n","        doc_id = sample['id_doc']\n","        \n","        # Padding char_start, char_end\n","        start = char_start + [0] * (self.num_mention - len(mentions))\n","        end = char_end + [0] * (self.num_mention - len(mentions))\n","\n","        # index mention shape: (num_mention, dim_char_x, dim_char_y)\n","        index_mentions = self.word2chars_mention(mentions, self.max_length_word, self.max_length_seq_char)\n","        index_mentions += [[[0] * self.max_length_seq_char] * self.max_length_word] * (self.num_mention - len(mentions))\n","\n","        # generate candidate from mention, mask denote addtional padding\n","        # candidates shape: (num_mention, num_candidate)\n","        # masks shape: (num_mention, num_candidate)\n","        # idx_entities shape: (num_mention, 1)\n","        # index_candidates shape: (num_mention, num_candidate, max_num_words, max_length_char)\n","        candidates, mask_candidates, idx_entities = self.generate_candidate(mentions, entities, self.num_candidate)\n","        index_candidates = self.word2chars_candidate(candidates, mask_candidates, self.max_length_word, self.max_length_seq_char)\n","        # index_candidates = torch.Tensor(index_candidates)\n","\n","        # Padding mask candidates\n","        mask_candidates += [[False] * self.num_candidate] * (self.num_mention - len(mentions))\n","\n","        # Padding idx_entities\n","        idx_entities += [-1] * (self.num_mention - len(mentions))\n","\n","        # create vector candidate padding\n","        # mention_padding = torch.zeros((self.num_mention - len(mentions), self.num_candidate, self.max_length_word, self.max_length_seq_char))\n","        # index_candidates = torch.cat((index_candidates, mention_padding), dim=0)\n","        index_candidates += [[[[0] * self.max_length_seq_char] * self.max_length_word] * self.num_candidate] * (self.num_mention -len(mentions))\n","        mask_mentions = [True if i < len(mentions) else False for i in range(self.num_mention)]\n","        \n","\n","        # index sentences shape: (max_length_seq_sence)\n","        index_sentence = torch.tensor(self.sence2word(sentence_id, self.max_length_seq_sence)).to(self.device)\n","        index_summary = torch.tensor(self.summary2word(doc_id, self.max_length_seq_sence)).to(self.device)\n","        index_candidates = torch.tensor(index_candidates).to(self.device)\n","        index_mentions = torch.tensor(index_mentions).to(self.device)\n","        mask_mentions = torch.tensor(mask_mentions).to(self.device)\n","        mask_candidates = torch.tensor(mask_candidates).to(self.device)\n","        # index_sentence = torch.tensor(index_sentence).to(self.device)\n","        start = torch.tensor(start).to(self.device)\n","        end = torch.tensor(end).to(self.device)\n","        idx_entities = torch.tensor(idx_entities).to(self.device)\n","\n","        return index_candidates, index_mentions, mask_mentions, mask_candidates, index_sentence, index_summary, start, end, idx_entities\n","\n","    def word2chars_mention(self, mentions, max_length_word, max_length_seq_char):\n","        index_chars = []\n","\n","        for mention in mentions:\n","            mention = mention.lower()\n","            mention = word_tokenize(mention)[:max_length_word]\n","            id_chars = [[self.char2id[c] if c in self.char2id else self.char2id['<unk>'] for c in list(w)[:max_length_seq_char]] for w in mention]\n","\n","\n","            ## Padding follow dim=1\n","            for i in range(len(id_chars)):\n","                id_chars[i] += [0] * (max_length_seq_char - len(id_chars[i]))\n","            \n","            ## Padding follow dim=0\n","            id_chars += [[0] * max_length_seq_char] * (max_length_word - len(id_chars))\n","            index_chars.append(id_chars)\n","\n","        return index_chars\n","    \n","    def word2chars_candidate(self, candidates, masks, max_length_word, max_length_seq_char):\n","        index_chars = []\n","        for candidate, mask in zip(candidates, masks):\n","            index_char = []\n","            for cand, msk in zip(candidate, mask):\n","                if msk:\n","                    cand = cand.lower()\n","                    cand = word_tokenize(cand)[:max_length_word]\n","                    id_chars = [[self.char2id[c] if c in self.char2id else self.char2id['<unk>'] for c in list(w)[:max_length_seq_char]] for w in cand]\n","                    ## Padding follow dim=1\n","                    for i in range(len(id_chars)):\n","                        id_chars[i] += [0] * (max_length_seq_char - len(id_chars[i]))\n","                    \n","                    ## Padding follow dim=0\n","                    id_chars += [[0] * max_length_seq_char] * (max_length_word - len(id_chars))\n","                    index_char.append(id_chars)\n","                else:\n","                    id_chars = [[0] * max_length_seq_char] * max_length_word\n","                    index_char.append(id_chars)\n","            \n","            index_chars.append(index_char)\n","        \n","        return index_chars\n","    \n","    def mention2word(self, mentions, max_length_word):\n","        index_words = []\n","        for mention in mentions:\n","            mention = mention.lower()\n","            mention = word_tokenize(mention)[:max_length_word]\n","\n","            id_word = [self.word2id[w] if w in self.word2id else self.word2id['unk'] for w in mention]\n","            id_word += [0] * (max_length_word - len(id_word))\n","        \n","            index_words.append(id_word)\n","        \n","        mask_mention_word = [[tok == 0 for tok in men] for men in index_words]\n","\n","        return index_words, mask_mention_word\n","    \n","    def candidate2word(self, candidates, masks, max_length_word):\n","        index_words = []\n","        for candidate, mask in zip(candidates, masks):\n","            index_word = []\n","            for cand, msk in zip(candidate, mask):\n","                if msk:\n","                    cand = cand.lower()\n","                    cand = word_tokenize(cand)[:max_length_word]\n","                    id_word = [self.word2id[w] if w in self.word2id else self.word2id['unk'] for w in candidate]\n","                    id_word += [0] * (max_length_word - len(id_word))\n","                    index_word.append(id_word)\n","                else:\n","                    id_word = [0] * max_length_word\n","                    index_word.append(id_word)\n","            \n","            index_words.append(index_word)\n","         \n","        mask_candidate_word = [[[c == 0 for c in cand] for cand in men] for men in index_words]\n","        \n","        return index_words, mask_candidate_word\n","\n","    def sence2word(self, sentence_id, max_length_seq_sence):\n","\n","        # print(self.sentences[sentence_id])\n","        sentence = self.sentences[sentence_id]\n","        sentence = sentence.lower()\n","        sentence = word_tokenize(sentence)[:max_length_seq_sence]\n","\n","\n","        id_sence = [self.word2id[s] if s in self.word2id else self.word2id['<unk>'] for s in sentence]\n","        id_sence += [0] * (max_length_seq_sence - len(id_sence))\n","\n","        # print(id_sence)\n","        return id_sence\n","    \n","    def summary2word(self, doc_id, max_length_seq_sence):\n","        \n","        # print(self.summary[doc_id])\n","        summary = self.summary[doc_id]\n","        summary = summary.lower()\n","        summary = word_tokenize(summary)[:max_length_seq_sence]\n","\n","\n","        id_sence = [self.word2id[s] if s in self.word2id else self.word2id['<unk>'] for s in summary]\n","        id_sence += [0] * (max_length_seq_sence - len(id_sence))\n","\n","        # print(id_sence)\n","        return id_sence\n","\n","    ### Generate candidate using available elasticsearch and probility\n","    def generate_candidate(self, mentions, entities, k=10):\n","\n","        candidates = []\n","        masks = []\n","        idx_entities = []\n","        for i, mention in enumerate(mentions):\n","            ## Get candidate from elasticsearch and probility\n","            candidate = []\n","            mention = mentions[i].lower()\n","            entity = entities[i].lower()\n","\n","            ## Get candidate:\n","            if mention in self.candidate_prob:\n","                cand_prob = [e[1] for e in self.candidate_prob[mention]]\n","            else:\n","                cand_prob = []\n","            \n","            if mention in self.candidate_elsearch:\n","                cand_elsearch = [e[1] for e in self.candidate_elsearch[mention]]\n","            else:\n","                if not self.train:\n","                    print(\"Vao\")\n","                    if self.search is not None:\n","                        cand_elsearch = [e for e in self.search_candidate(mention)][:k-1]\n","                    else:\n","                        cand_elsearch = []\n","                    \n","                    print(cand_elsearch)\n","                else:\n","                    cand_elsearch = []\n","                \n","                # cand_elsearch = []\n","\n","            ## Create candidate\n","            if len(cand_prob) + len(cand_elsearch) <= k:\n","                candidate.extend(cand_prob)\n","                candidate.extend(cand_elsearch)\n","                candidate = candidate[:k-1]\n","            elif len(cand_prob) >= k:\n","                candidate.extend(cand_prob[:k-1])\n","            else:\n","                candidate.extend(cand_prob)\n","                for cand in cand_elsearch:\n","                    if len(candidate) >= k - 1:\n","                        break\n","                    if cand not in candidate:\n","                        candidate.append(cand)\n","\n","\n","            candidate = list(set(candidate))\n","            ## Check entity in candidate, if not in, ==> append entity into candidate ==> label\n","            if entity not in candidate:\n","                candidate.append(entity)     \n","\n","            pad = ['<pad>'] * (k - len(candidate))\n","            candidate = candidate + pad\n","            mask = [c != '<pad>' for c in candidate]\n","\n","            idx_entity = candidate.index(entity)\n","\n","            candidates.append(candidate)\n","            masks.append(mask)\n","            idx_entities.append(idx_entity)\n","        \n","        return candidates, masks, idx_entities\n","    def search_candidate(self, m):\n","        candidate = set()\n","        str_search = re.sub(r'[\\/\\\\~\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\-\\+\\[\\]\\\"\\'\\:\\`\\{\\}]', ' ', m)\n","        tmp = self.search.query('query_string', query=str_search, fields=['redirects^1','alias^1'])\n","        res = tmp.execute()\n","        for res_entity, res_hit in zip(res, res.hits):\n","            candidate.add(res_entity.entity)\n","        \n","        return candidate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"166-O8KcZaPL","executionInfo":{"status":"ok","timestamp":1648202155139,"user_tz":-420,"elapsed":1233,"user":{"displayName":"Nghĩa Ngọ Doãn","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13982214725351301935"}},"outputId":"5e660f0a-a628-4753-b375-579754d6d43f"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","num_mentions = 3\n","num_candidates = 15\n","max_length_seq_char = 32\n","max_length_seq_sence = 30\n","max_length_word = 8\n","\n","batch_size, epoch = 32, 50\n","output_dim_word = 100 ## dim of ouput mention, candidate when that pass averge_mention\n","\n","## argument encoder\n","d_model, n_layers, n_heads, d_ff, clip, pad_idx = 256, 6, 4, 512, 1.0, 0\n","dropout_rate = 0.2\n","\n","## argument lstm\n","input_dim_lstm, hidden_size, bidirection, num_layers = 300, 50, True, 1 \n","\n","## argument mlp embed\n","input_dim_mlp_embed, output_dim_mlp_embed, num_hidden_mlp_embed = max_length_word * (hidden_size*2)*2, 100, 256\n","\n","## argument mlp score\n","input_dim_mlp_score, output_dim_mlp_score, num_hidden_mlp_score1, num_hidden_mlp_score2 = d_model*2 + output_dim_mlp_embed + max_length_word * output_dim_word * 3 + 1, 1, 256, 10\n","\n","word2id, char2id, sentences, candidate_elsearch, candidate_prob, samples, samples_test, sentences_test, summary = load_data()\n","vocab_chars = len(char2id)\n","vocab_words = len(word2id)\n","dim_char = hidden_size * 2\n","\n","encoder = Encoder(vocab_words, d_model, n_layers, n_heads, d_ff, pad_idx, dropout_rate)\n","char_embed = EmbedCharLayer(vocab_chars, max_length_seq_char, input_dim_lstm, hidden_size, num_layers, device, dropout_rate, bidirection)\n","sentence_embed = EmbedSentenceLayer(vocab_words, input_dim_lstm, hidden_size, num_layers, device, dropout_rate, bidirection)\n","mlp_embed = MLPEmbedingLayer(input_dim_mlp_embed, output_dim_mlp_embed, num_hidden_mlp_embed, dropout_rate)\n","mlp_score = MLPScoreLayer(input_dim_mlp_score, output_dim_mlp_score, num_hidden_mlp_score1, num_hidden_mlp_score2, dropout_rate)\n","\n","\n","model = Entity_Linking(encoder, char_embed, sentence_embed, mlp_embed, mlp_score, num_mentions, num_candidates, max_length_seq_char, max_length_seq_sence, max_length_word, dim_char, output_dim_word, device)\n","loss = MyMarginLoss()\n","optimizer =  torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9, weight_decay=1e-5)\n","model.to(device)\n","\n","print(\"Load model ....\")\n","model.load_state_dict(torch.load('./model/model_el.pt'))\n","print(\"Complete load...\")\n","\n"],"metadata":{"id":"mwN-PhwhAPmP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset = CustomerDataset(samples_test, sentences_test, summary, num_mentions, num_candidates, max_length_seq_char, max_length_seq_sence, max_length_word, char2id, word2id, candidate_prob, candidate_elsearch, device, s, False)\n","\n","test_dataloader = dataloader(test_dataset, batch_size=2, train=False)\n","\n","loss_test, acc_test = evaluate(test_dataloader)\n","    \n","print(f\"Accuracy test = {acc_test}, Loss Test = {loss_test}\")"],"metadata":{"id":"Eg2tRieAdRXl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"iLlZZ6gDk2j8"},"execution_count":null,"outputs":[]}]}